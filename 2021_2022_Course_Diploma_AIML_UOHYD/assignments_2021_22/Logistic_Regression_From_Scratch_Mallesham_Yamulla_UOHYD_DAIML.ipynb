{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14lIZBi-bFar"
      },
      "source": [
        "## 1. Logistic Regression with L1 regularization from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDN6DGw3bT8l"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "  1. Read in the train_data.\n",
        "  2. Vectorize train_data and test_data using sklearns built in tfidf vectorizer.\n",
        "  3. Ignore unigrams and make use of both **bigrams & trigrams** and also limit the **max features** to **2000** and **minimum document frequency** to **10**.\n",
        "  4. After the tfidf vectors are generated as mentioned above, next task is to column standardize your data.\n",
        "  5. We want you to write in comments in your code, the reason you think for standardizing the data in the above step.\n",
        "  6. You can use sklearn StandardScaler to column standardize your data.\n",
        "  7. Write a function to initialise your weights & bias. And then run its corresponding grader function.\n",
        "  8. Write a custom function to calculate sigmoid of a value. And then run its corresponding grader function to cross check your implementation of sigmoid function.\n",
        "  9. Write a custom function to compute the total loss as the sum of log loss and l1 regularization loss based on true labels and predicted labels and weights. And you can crosscheck your implementation with its corresponding grader.\n",
        "  10. Write a function to compute gradients for your weights and bias terms, which you have to make use of in updating your weights and bias while training your model.\n",
        "  11. Implement a custom train function of logistic regression, wherein you take in the following inputs:\n",
        "        * **X_train** which will be your vectorized text data\n",
        "        * **y_train** which are the labels for your train data\n",
        "        * **alpha** = 0.0001 which is the regularization factor (Î»)\n",
        "        * **eta0** = 0.0001 which will be the learning rate   \n",
        "        * **tolerance** = 0.001\n",
        "        \n",
        "  12. In the custom train function you should make use of a custom SGD function to update the weights and bias terms for **each** of your inputs.\n",
        "  13. The custom SGD implemented in the above train function for updating the weights and bias terms should run for many epochs until the difference in loss between two consecutive epochs is less than tolerance.\n",
        "\n",
        "  14. Here one epoch means a complete iteration of your entire train data.\n",
        "  15. Your train function should return the follwing:\n",
        "        * the number of epochs it took to complete the training\n",
        "        * train loss for all epochs\n",
        "        * the values for final weights and bias terms.\n",
        "        \n",
        "  16. Now run the grader function to check whether the weights and bias obtained from your custom implementation are close enough to that of sklearns implementation.\n",
        "  17. Next write a custom predict function which takes in as input the weights and bias values that you computed in your train function, and also takes in the test standardized data as input to predict its labels.\n",
        "  18. Now run the grader function to check the accuracy of your predictions.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0X7xnuGla6r"
      },
      "source": [
        "### Data wrangling and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMGdF1QGjjE8"
      },
      "source": [
        "# Loading necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import operator\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "import altair as alt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWFQEhUqkBk8"
      },
      "source": [
        "# read in the dataset\n",
        "comments_raw_df = pd.read_csv('https://raw.githubusercontent.com/myamullaciencia/open_into_datos/master/logistic_regression_assignment_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "oz3UabrmkLLA",
        "outputId": "d632a7a5-0f10-47ab-e4df-cdb20b9ae61f"
      },
      "source": [
        "# glance at comments dataframe\n",
        "comments_raw_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category                                               text\n",
              "0         0  worldcom boss  left books alone  former worldc...\n",
              "1         1  tigers wary of farrell  gamble  leicester say ...\n",
              "2         1  yeading face newcastle in fa cup premiership s...\n",
              "3         1  henman hopes ended in dubai third seed tim hen...\n",
              "4         1  wilkinson fit to face edinburgh england captai..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smFKFjiQkfnr",
        "outputId": "6d143d29-dd3a-426a-cd0b-d7bf72808613"
      },
      "source": [
        "# levels of category field\n",
        "comments_raw_df.value_counts('category')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "1    509\n",
              "0    508\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0B01fMcktup"
      },
      "source": [
        "# creating pandas sereis with text and category fields\n",
        "comments_df_text = comments_raw_df['text']\n",
        "comments_df_category = comments_raw_df['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9kQJpn_k6z9"
      },
      "source": [
        "# Creating training and testing datasets\n",
        "train_comments_text,test_comments_text,train_comments_cat,test_comments_cat = train_test_split(comments_df_text,comments_df_category,random_state=100, stratify=comments_df_category, test_size=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4_TYlE6lkkX",
        "outputId": "bb871583-5eb2-4923-cc8a-f588a8524009"
      },
      "source": [
        "print(f'1.X_Training comments set shape is {train_comments_text.shape}\\n2.X_Testing comments set shape is {test_comments_text.shape}\\n3.Y_Training category set shape {train_comments_cat.shape}\\n4.Y_Testing category set shape {test_comments_cat.shape}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.X_Training comments set shape is (1006,)\n",
            "2.X_Testing comments set shape is (11,)\n",
            "3.Y_Training category set shape (1006,)\n",
            "4.Y_Testing category set shape (11,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zafeCcEBnEUB"
      },
      "source": [
        "# Define text tfidf vectorizer\n",
        "LR_text_vectorizer = TfidfVectorizer(ngram_range=(2,3),max_features=2000,min_df=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT6hFeQWng9e"
      },
      "source": [
        "# fitting the text vectorizer on training set\n",
        "train_comments_vec_fit = LR_text_vectorizer.fit(train_comments_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zroG2r6wn_oG"
      },
      "source": [
        "# creating training and testing vectors\n",
        "train_comments_vectors = train_comments_vec_fit.transform(train_comments_text)\n",
        "test_comments_vectors = train_comments_vec_fit.transform(test_comments_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahslso1GoSAe",
        "outputId": "add5799b-b4c4-42b2-cb0e-58f5b7934cfe"
      },
      "source": [
        "print(f'The training and testing vectors shape as: {train_comments_vectors.shape , test_comments_vectors.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training and testing vectors shape as: ((1006, 2000), (11, 2000))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf-mUeF_otWI"
      },
      "source": [
        "# Standardization initializer\n",
        "txt_scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY2WFNo4jk2E"
      },
      "source": [
        "**Column standardization:**\n",
        "\n",
        "The reason for standardizing the predictor variables as -\n",
        "\n",
        "1. To bring all the predictor varibles which have different scales on to the same scales.\n",
        "\n",
        "2. Used to improve the numerical stability of model calculations.\n",
        "\n",
        "3. Regularizations deals with a penalty on the magnitude of the coefficients caluclated on each predictor, here the scale of predictor will affect how much penalty is applied on their coefficients and the predictors with large variances will have small coefficients, as a result of it they will be less penalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33VpwLJ6rqu8"
      },
      "source": [
        "# train and test standardized vectors\n",
        "train_comments_std_vec = txt_scaler.fit_transform(train_comments_vectors.toarray())\n",
        "test_comments_std_vec = txt_scaler.fit_transform(test_comments_vectors.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSAifwRLjuOV"
      },
      "source": [
        "### Custom function implementations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNyLfqK6sRhg"
      },
      "source": [
        "def initialize_weights_bias(dim):\n",
        "    ''' In this function, we will initialize our weights and bias terms'''\n",
        "\n",
        "    # Initialize the weights to zeros array of (dim) dimensions. Here dim will be the number of features of your tfidf vectorizer output.\n",
        "    # You can initialize the weight terms with zeros.\n",
        "    w=np.full((dim),0)\n",
        "    # Initialize bias term to zero\n",
        "    b=0.0\n",
        "    return w,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhffoimDtC-J"
      },
      "source": [
        "def custom_sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # Compute sigmoid(z) and return its value.\n",
        "    # Write your code below.\n",
        "    sigmoid= 1.0/(1 + np.exp(-z))\n",
        "    return sigmoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyX_g67chgUA"
      },
      "source": [
        "def custom_loss(y_true, y_pred, alpha, w):\n",
        "    '''In this function, we will compute total loss which is [(logloss) + (alpha * L1regularization loss)] '''\n",
        "    log_loss = -1 * np.mean(y_true*(np.log10(y_pred)) + (1-y_true)*np.log10(1-y_pred))\n",
        "    l1_loss = sum(abs(w))\n",
        "    total_loss = log_loss + alpha*l1_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDt4yzYH1nQO"
      },
      "source": [
        "def gradient_dw(x, y, w, b, alpha, N):\n",
        "    '''In this function, we will compute the gardient w.r.t. w '''\n",
        "\n",
        "    # Write your code below.+ (alpha*(w+(1e-5))/abs(w+(1e-5)))/N\n",
        "    part_1 = np.dot(custom_sigmoid(np.dot(x,w.T) + b)-y,x)\n",
        "    part_2 = np.dot((alpha/N),(w+1e-5/abs(w+1e-5)))\n",
        "    dw = part_1+part_2\n",
        "\n",
        "    return dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0PKORurzR3E"
      },
      "source": [
        "def gradient_db(x, y, w, b):\n",
        "    '''In this function, we will compute the gardient w.r.t. b '''\n",
        "    sig = custom_sigmoid(np.dot(x,w.T)+b)\n",
        "    db = sig - y\n",
        "    return db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZNjD--6LIyV"
      },
      "source": [
        "def custom_train(X_train, y_train,alpha, eta0,tolerance):\n",
        "  \"\"\"\n",
        "  In this function we will compute optimal values for weights and bias terms on\n",
        "  the train data.\n",
        "\n",
        "  Here eta0 is the learning rate and alpha is the regularization term.\n",
        "  \"\"\"\n",
        "  train_loss=[]\n",
        "\n",
        "  # 1. Initalize the weights (call the initialize_weights(X_train[0]) function)\n",
        "  w,b = initialize_weights_bias(X_train.shape[1])\n",
        "  print(f'first:{b}')\n",
        "  # 2. Repeat For many epochs until condition \"e\"  fails\n",
        "          # a) for every data point(X_train,y_train)\n",
        "                # compute gradient w.r.to w (call the gradient_dw() function)\n",
        "                # compute gradient w.r.to b (call the gradient_db() function)\n",
        "                # update w, b using the above eqns\n",
        "          # b) predict the output of x_train[for all data points in X_train] using w,b\n",
        "          # c) compute the loss between predicted and actual values (call the loss function)\n",
        "          # d) store all the train loss values in a list\n",
        "          # e) Compare previous loss and current loss, if the difference between loss is not more than or equal to the tolerance, stop the process and return w,b\n",
        "\n",
        "  # 3. Return the values of weights, bias, train_loss and num_epochs\n",
        "  num_epochs=0\n",
        "  while True:\n",
        "      num_epochs+=1\n",
        "      dw = gradient_dw(X_train,y_train,w,b,alpha,X_train.shape[0])\n",
        "      db = gradient_db(X_train,y_train,w,b)\n",
        "      y_pred = custom_sigmoid(np.dot(w,X_train.T)+b)\n",
        "      loss = custom_loss(y_train, y_pred, alpha, w)\n",
        "      train_loss.append(loss)\n",
        "      w=w-(np.dot(eta0,dw))\n",
        "      b=b-(np.dot(eta0,db))\n",
        "      print(b)\n",
        "      if len(train_loss)>=2:\n",
        "          for _ in range(len(train_loss)):\n",
        "              idx=len(train_loss)\n",
        "              nxt=idx-1\n",
        "              prev = nxt-1\n",
        "              diff = train_loss[prev]-train_loss[nxt]\n",
        "          if diff < tolerance:\n",
        "              break\n",
        "  return w,b,train_loss,num_epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHxQ-nm3EQLS"
      },
      "source": [
        "w,b,train_loss,epochs = c\n",
        "alt.Chart(pd.DataFrame({'epochs':range(epochs),'train_loss':train_loss})).mark_line().encode(\n",
        "    x='epochs',\n",
        "    y='train_loss'\n",
        ").properties(\n",
        "    title='epoch vs loss'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_-olbN5E37b"
      },
      "source": [
        "def predict(w,b, X):\n",
        "    '''function to predict label given weights, bias and standardized data'''\n",
        "    predictions = custom_sigmoid(np.dot(w,X.T)+np.mean(b))\n",
        "    pred_z = np.array(predictions)\n",
        "    y_pred_prob=[]\n",
        "    for z in pred_z:\n",
        "        if z>=0.5:\n",
        "            y_pred_prob.append(1)\n",
        "        else:\n",
        "            y_pred_prob.append(0)\n",
        "    return np.array(y_pred_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBZZkyE7055x"
      },
      "source": [
        "### Grader functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-tukErKtMoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f80be77-481b-4ea3-f78d-574e8bb01fb6"
      },
      "source": [
        "def grader_sigmoid(z):\n",
        "  val = custom_sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "\n",
        "grader_2 = grader_sigmoid(2)\n",
        "print(\"Grader_2 Status : \", grader_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grader_2 Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQorN2tKoOgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc005cc-9856-4658-c40f-881424e24e98"
      },
      "source": [
        "# Grader function to check the initialization of your weights and bias terms.\n",
        "\n",
        "def grader_weights_bias(w,b):\n",
        "  assert((len(w)==2000) and b==0)\n",
        "  return True\n",
        "\n",
        "dim = 2000\n",
        "w,b = initialize_weights_bias(dim)\n",
        "grader_1 = grader_weights_bias(w,b)\n",
        "print(\"Grader_1 Status : \", grader_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grader_1 Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpq6EQnFrpTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8c10e0-9733-4054-86c8-5fb1ce56a373"
      },
      "source": [
        "# Grader function to check the implementaiton of logloss\n",
        "def grader_loss():\n",
        "  true_values = np.array([1,1,0,1,0])\n",
        "  pred_values = np.array([0.9,0.8,0.1,0.8,0.2])\n",
        "  w= np.array([0.1]*10)\n",
        "  alpha= 0.0001\n",
        "  loss = custom_loss(true_values, pred_values,alpha,w)\n",
        "  assert(loss==0.07644900402910389+0.0001*10*0.1)\n",
        "  return True\n",
        "\n",
        "\n",
        "grader_3 = grader_loss()\n",
        "print(\"Grader_3 Status : \", grader_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grader_3 Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZoqz2zVBdM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66975f58-fe7a-4ff6-81ba-db11847093b5"
      },
      "source": [
        "def grader_weights_bias():\n",
        "  # fitting sklearn SGD classifier\n",
        "  clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l1', tol=1e-3, learning_rate='constant')\n",
        "  clf.fit(train_comments_std_vec,train_comments_cat.values)\n",
        "  model_coef= clf.coef_[0]\n",
        "\n",
        "  # fitting custom train with same learning rate, regularization and tolerance as of sklearn\n",
        "  w,b,_,epoch = custom_train(train_comments_std_vec,train_comments_cat.values, 0.0001,0.0001,0.001)\n",
        "  #w,b,_,epoch = custom_train(train_vectors_stand, train_category.values, 0.0001,0.0001,0.001)\n",
        "\n",
        "  # checking whether the weights and bias returned by both the implementations are closer\n",
        "  assert((not (w-model_coef>0.02).any())==True)\n",
        "  assert(not (np.mean(b)-clf.intercept_>0.02)==True)\n",
        "\n",
        "  return True\n",
        "\n",
        "grader_4 = grader_weights_bias()\n",
        "print(\"Grader_4 Status : \", grader_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grader_4 Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQluAo3nFWD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f497510-ac4c-4aa9-aa56-b22932a721fc"
      },
      "source": [
        "def grader_predict():\n",
        "  ''' grader to check the test accuracy'''\n",
        "  w,b,_,_ = custom_train(train_comments_std_vec,train_comments_cat.values, 0.0001,0.0001,0.001)\n",
        "  test_preds= predict(w,b,test_comments_std_vec)\n",
        "  test_accuracy= (np.sum(test_comments_cat==test_preds)/len(test_preds))*100\n",
        "  if(test_accuracy>=90):\n",
        "    print(\"Success!!!\",test_accuracy)\n",
        "  else:\n",
        "    print(\"Failed! \\n Test accuracy = \", test_accuracy)\n",
        "  return\n",
        "\n",
        "grader_predict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!!! 100.0\n"
          ]
        }
      ]
    }
  ]
}